## GIN

#### 1. 本文结构

1. 本文结构
2. 基本信息
3. 摘要内容
4. 图表信息
5. 全文内容
6. 代码分析
7. 读后思考

#### 2. 基本信息

- 标题: How powerful are Graph Neural Networks?
- 作者: Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka
- 会议: ICLR2019
- 代码链接: https://github.com/weihua916/powerful-gnns

#### 3. 摘要内容

GNNs是一个对于图来说有效的表示学习的框架。GNNs用一种邻居聚集模式，具体来说就是每个节点的表示向量被递归地聚集并且转换它邻居节点的表示向量。血多GNN的变种被提出来解决节点分类或者图分类，并且达到了SOTA的效果。然而，尽管GNNs改变了图表示学习，但是对于它们的表示属性和限制我们了解的还是很少。这里，为了分析GNNs捕获不同图结构的能力我们提出一个理论框架。我们的结果刻画了流行的GNN变种的区分能力，包括GCN和GraphSAGE，并且展示了它们不能够学会区分特定的简单图结构。我们之后开发了一个简单的但是很有表示能力的GNN，并且它跟WL graph同构测试一样有力。我们在一些图分类benchmark上做了测试，然后说明我们的模型达到了SOTA。

#### 4. 图表信息

![image-20200914164613821](https://raw.githubusercontent.com/TimeLovercc/img/master/image-20200914164613821.png)

Figure 1: 我们理论框架的总览。中间的子树结构用来区分不同的图。如果一个GNN的聚集函数捕获节点邻居的完全多重集，GNN能以递归的方式捕捉生根的字数并且达到**WL test**的效果。

![image-20200914165410012](https://raw.githubusercontent.com/TimeLovercc/img/master/image-20200914165410012.png)

Figure 2: 在一个多重集上sum, mean 和max聚集的表达能力。左边的面板展示了输入的多重集, 也就是说要聚集的网络邻居。接下来的三个panel说明了不同聚集器能捕获的不同层面：sum能够捕获整个多重集，mean能捕获给定类型的ili，然后max忽略了多样性。

![image-20200914165922908](https://raw.githubusercontent.com/TimeLovercc/img/master/image-20200914165922908.png)

Figure 3: mean和max聚集起不能区分的结构。在这两个图种，节点v和v'得到了相同的嵌入尽管它们的结构不同。图2说明了压缩多重集方式不同导致不能区分。

![image-20200914170144087](https://raw.githubusercontent.com/TimeLovercc/img/master/image-20200914170144087.png)

Figure 4: GINs和其它GNN变种和WL子树核在不同训练集上的表现。

![image-20200914170256216](https://raw.githubusercontent.com/TimeLovercc/img/master/image-20200914170256216.png)

Table 1: 效果比较好。

#### 5. 全文内容

##### 1. Introduction

文章主要四个方面内容，GNN的区分性能、区分的条件、不能区分的情形、一个简单的结构。

##### 2. Preliminaries

GNN的特征可以用三部分的组合表述，聚集、组合和读出。

WL子树核。

##### 3. Theoretical framework: overview

必须是单射，不能把不同的节点群映射成相同的。

##### 4. Building powerful graph neural networks

GNN不仅能区分不同结构，还能把相似结构映射到相似位置并且找出图结构之间的依赖。

通过加一个小的微扰因子，让模型单射。

##### 5. Less powerful but still interesting GNNs

Mean适合在结构不重复的图中找，max适合找到"skeleton"。

##### 6. Other related work

GIN理论基础强，其它的GNNs理论基础不强。

##### 7. Experiments

细节。总之效果很好。

##### 8. Conclusion

最强大的GNN。未来方向是超过邻居聚集，找到在图上学习的更好的方式。

#### 6. 代码分析

![image-20200915131710836](https://raw.githubusercontent.com/TimeLovercc/img/master/image-20200915131710836.png)

其实比较简单，模型就是将max-pool或者mean-pool那部分替换成了自己的方法。

#### 7. 读后思考

图跟图像、语音这类数据很不一样，它是一点不能错的，精确的输入。我们处理这类数据要用精确的、更理论的方法。这也是一个机遇，或者是理解神经网络的一个入口。

Weisfeiler-lehman graph kernels

GAT